{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wwhHInG39Ir"
   },
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "# load pretrained tokenizer, call it with dataset\n",
    "# build PyTorch dataset with encodings\n",
    "# load pretrained model\n",
    "# load trainer and train int / native pytorch trianing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAZp_gTA8rPL",
    "outputId": "9672c07d-62ce-4768-b4a7-1a2c01aa777a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.29.0 in c:\\users\\ameen\\anaconda3\\lib\\site-packages (4.29.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ameen\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\ameen\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (0.14.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ameen\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (2.29.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ameen\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ameen\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (1.24.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ameen\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\ameen\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (0.11.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ameen\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ameen\\anaconda3\\lib\\site-packages (from transformers==4.29.0) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ameen\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.29.0) (4.5.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ameen\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.29.0) (2022.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ameen\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.29.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ameen\\anaconda3\\lib\\site-packages (from requests->transformers==4.29.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ameen\\anaconda3\\lib\\site-packages (from requests->transformers==4.29.0) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ameen\\anaconda3\\lib\\site-packages (from requests->transformers==4.29.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ameen\\anaconda3\\lib\\site-packages (from requests->transformers==4.29.0) (1.26.15)\n",
      "Collecting git+https://github.com/huggingface/accelerate\n",
      "  Cloning https://github.com/huggingface/accelerate to c:\\users\\ameen\\appdata\\local\\temp\\pip-req-build-i1bb8yok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Error [WinError 2] The system cannot find the file specified while executing command git version\n",
      "ERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH?\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.29.0\n",
    "!pip install git+https://github.com/huggingface/accelerate\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2E-WZFWo9gq3",
    "outputId": "67c19e67-fe99-4cf1-c90e-2d3c239fe7f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "C:\\Users\\ameen\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/2:  45%|████▌     | 5/11 [2:33:16<3:02:23, 1823.91s/it, Loss=5.49]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AdamW\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 16\n",
    "max_input_length = 128\n",
    "max_output_length = 128\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 2\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"K:\\Github\\ActasGPT\\model\\chatgpt-prompts\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"K:\\Github\\ActasGPT\\model\\chatgpt-prompts\", from_tf=True)\n",
    "\n",
    "# Move model to GPU device\n",
    "model.to(device)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"K:\\Github\\ActasGPT\\model\\prompts.csv\")\n",
    "\n",
    "# Define the dataset class\n",
    "class ChatGPTDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_input_length, max_output_length):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_text = self.data['act'][index]\n",
    "        target_text = self.data['prompt'][index]\n",
    "\n",
    "        input_encoding = self.tokenizer.batch_encode_plus(\n",
    "            [input_text],\n",
    "            max_length=self.max_input_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target_encoding = self.tokenizer.batch_encode_plus(\n",
    "            [target_text],\n",
    "            max_length=self.max_output_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = input_encoding['input_ids'].squeeze().to(device)\n",
    "        input_attention_mask = input_encoding['attention_mask'].squeeze().to(device)\n",
    "        target_ids = target_encoding['input_ids'].squeeze().to(device)\n",
    "        target_attention_mask = target_encoding['attention_mask'].squeeze().to(device)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': input_attention_mask,\n",
    "            'target_ids': target_ids,\n",
    "            'target_attention_mask': target_attention_mask\n",
    "        }\n",
    "\n",
    "# Prepare the dataset\n",
    "dataset = ChatGPTDataset(df, tokenizer, max_input_length, max_output_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Prepare the optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        target_ids = batch['target_ids']\n",
    "        target_attention_mask = batch['target_attention_mask']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': target_ids,\n",
    "            'decoder_attention_mask': target_attention_mask\n",
    "        }\n",
    "\n",
    "        # Move inputs to GPU device\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss}\")\n",
    "\n",
    "# Because of the large size of the model, the Jupyter kernel will continue running the cell and generate the output even if the notebook is closed, but it won't display the results.    \n",
    "model.save_pretrained('chatgpt-prompts')\n",
    "tokenizer.save_pretrained('chatgpt-prompts')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
